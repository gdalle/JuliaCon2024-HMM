---
title: "Fast and generic Hidden Markov Models"
author:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    email: guillaume.dalle@epfl.ch
    affiliation: 
      - name: École Polytechnique Fédérale de Lausanne
      - department: IdePHICS, INDY & SPOC laboratories
date: "2024-07-12"
bibliography: HMM.bib
engine: julia
format:
  revealjs:
    slide-number: true
    overview: true
    code-line-numbers: false
    scrollable: true
    theme: [default, custom.scss]
execute:
    echo: true
---

# Introduction

## About me

- Aka [`gdalle`](https://github.com/gdalle) on GitHub & Discourse
- Postdoctoral researcher at EPFL (Switzerland)
- Interested in machine learning ([JuliaDiff](https://juliadiff.org/)) and optimization ([JuliaGraphs](https://juliagraphs.org/))
- Check out my website: <https://gdalle.github.io/>
- Other talk: [_Gradients for everyone: a quick guide to autodiff in Julia_](https://pretalx.com/juliacon2024/talk/YGZYWK/)

## What is a Hidden Markov Model?

:::: {.columns}

::: {.column width=65%}
- Statistical model for a temporal process
- Hidden state $X_t$ evolves through time
- Observations $Y_t$ depend on the state
- Basically a noisy Markov chain
:::

::: {.column width="30%"}
![The first Markov chain [@vonhilgersFiveGreatestApplications2006]](./images/eugene_oneguin.png){height=400}
:::

::::

## Applications

- Speech processing
- Bioinformatics
- Predictive maintenance

# The math

## Some notation

:::: {.columns}

::: {.column width="45%"}
- $N$ states, $M$ observations
- Initial probabilities $p$
- Transition matrix $A$
- Emission matrix $B$
:::

::: {.column width="5%"}
:::

::: {.column width="50%"}
$$\mathbb{P}(X_0 = i) = p_i$$
$$\mathbb{P}(X_{t} = j | X_{t-1} = i) = A_{i,j}$$
$$\mathbb{P}(Y_{t} = k | X_t = i) = B_{i,k}$$
:::

::::

## Example: my JuliaCon attendance

:::: {.columns}

::: {.column width="45%"}
```{mermaid}
%%| echo: false
stateDiagram-v2
  direction LR
  state "JuliaCon\nonline" as online
  state "JuliaCon\nin Europe" as europe
  state "JuliaCon\noverseas" as overseas
  online --> overseas: 1
  overseas --> online: 1
  overseas --> europe: 2
  europe --> overseas: 1
  overseas --> overseas: 3
  online --> online: 2
```

$$p = \begin{pmatrix} 0 & 0 & 1 \end{pmatrix}$$

$$A = \begin{pmatrix} 2/3 & 1/3 & 0 \\ 1/6 & 3/6 & 2/6 \\ 0 & 1 & 0 \end{pmatrix}$$
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{mermaid}
%%| echo: false
stateDiagram-v2
  state "JuliaCon\nonline" as online
  state "JuliaCon\nin Europe" as europe
  state "JuliaCon\noverseas" as overseas
  state "I'm attending" as attending
  state "I'm missing" as missing
  online --> attending: 3
  overseas --> missing: 6
  europe --> attending: 1
  europe --> missing: 1
```

$$B = \begin{pmatrix} 1 & 0 \\ 1/2 & 1/2 \\ 0 & 1 \end{pmatrix}$$
:::

::::

::: {.notes}
2024: Eindhoven
2023: Cambridge MA
2022: Online
2021: Online
2020: Online
2019: Baltimore MD
2018: London
2017: Berkeley CA
2016: Cambridge MA
2015: Cambridge MA
2014: Cambridge MA
:::

## Main algorithms

Given an HMM, one may want to:

- Observation likelihood $\mathbb{P}_\theta(Y_{1:T})$ $\implies$ Forward
- Most likely states $\underset{X_{1:T}}{\max} \mathbb{P}_\theta(X_{1:T} | Y_{1:T})$ $\implies$ Viterbi
- Best parameters $\underset{\theta}{\max} \mathbb{P}_\theta(Y_{1:T})$ $\implies$ Baum-Welch

See the tutorial by @rabinerTutorialHiddenMarkov1989 for details.

# The code

## Basics

Model creation:
```{julia}
using HiddenMarkovModels, Distributions, Random
rng = Random.default_rng()

p = [0.0, 0.0, 1.0]
A = [2/3 1/3 0; 1/6 3/6 2/6; 0 1 0]
B = [1 0; 1/2 1/2; 0 1]
dists = Categorical.(Vector.(eachrow(B)))
model = HMM(p, A, dists);
```
Simulation:
```{julia}
rand(rng, model, 5)
```

## Inference

JuliaCon attendance data:
```{julia}
#| echo: false
state_meaning(i) = if i == 1
  return :online
elseif i == 2
  return :overseas
elseif i == 3
  return :europe
end

obs_meaning(k) = if k == 1
  return :attending
elseif k == 2
  return :missing
end;
```
```{julia}
state_seq = [2, 2, 2, 2, 3, 2, 1, 1, 1, 2, 3]
obs_seq = [2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1];
```

Likelihood:

```{julia}
logdensityof(model, obs_seq)
```

State sequence:

```{julia}
best_state_seq, _ = viterbi(model, obs_seq)
```

## Learning

:::: {.columns}

::: {.column width="60%"}

```{julia}
#| echo: false
using HiddenMarkovModels: rand_prob_vec, rand_trans_mat
```

```{julia}
long_obs_seq = rand(rng, model, 1000).obs_seq
p0, A0 = rand_prob_vec(3), rand_trans_mat(3)
model_init = HMM(p0, A0, dists)

model_est, logL = baum_welch(
  model, long_obs_seq);
```

```{julia}
transition_matrix(model_est)
```

```{julia}
probs.(obs_distributions(model_est))
```
:::

::: {.column width="40%"}
```{julia}
#| echo: false
using CairoMakie
fig = Figure(size=(400, 400))
ax = Axis(fig[1, 1], xlabel="Baum-Welch iteration", ylabel="Loglikelihood")
lines!(ax, logL, linewidth=2)
fig
```
:::

::::

## Why a new package?

- Python: [`hmmlearn`](https://github.com/hmmlearn/hmmlearn), [`pomegranate`](https://pomegranate.readthedocs.io/), [`dynamax`](https://github.com/probml/dynamax)
- Julia: [HMMBase.jl](https://github.com/maxmouchet/HMMBase.jl), based on [Distributions.jl](https://github.com/JuliaStats/Distributions.jl)

Need for a generic, fast and reliable library.

# More generality

## Number types

- ~~I need `Float64` everywhere or I get really stressed out~~
- I can handle any real number they throw at me

::::: {.columns}

:::: {.column width="50%"}

```{julia}
import ForwardDiff

f(μ) = logdensityof(
  HMM(
    [0.9, 0.1], [0.7 0.3; 0.2 0.8],
    Normal.(μ, ones(2))
  ),
  [-1.0, -1.3, -0.6, 1.4, 1.2, 0.9]
)

ForwardDiff.gradient(f, [-2.0, 2.0])
```

::::

:::: {.column width="50%"}

::: {.callout-tip}
## Benefits

Variable precision, automatic differentiation, logarithmic storage.
:::

::: {.callout-important}
## Challenges

Parametric types everywhere, handling promotion correctly.
:::

::::

:::::

## Matrix types

- ~~The only good matrix is a dense `Matrix`~~
- Who's afraid of the big bad `SparseMatrixCSC`?

::::: {.columns}

:::: {.column width="50%"}

```{julia}
using SparseArrays

model_sp = HMM(p, sparse(A), dists)
model_sp_est, _ = baum_welch(
  model_sp, long_obs_seq
)
transition_matrix(model_sp_est)
```

::::

:::: {.column width="50%"}

::: {.callout-tip}
## Benefits

Large state spaces, realistic transition structures.
:::

::: {.callout-important}
## Challenges

Generic updates during parameter estimation.
:::

::::

:::::

## Observation types

- ~~Distributions.jl is my whole life~~
- Give me a sampler + loglikelihood and let's go

::::: {.columns}

:::: {.column width="50%"}

```{julia}
struct MyDist
  length::Int
end
```
```{julia}
#| echo: false
import DensityInterface
using Random: AbstractRNG
DensityInterface.DensityKind(::MyDist) = DensityInterface.IsDensity()
```
```{julia}
Base.rand(
  rng::AbstractRNG, dist::MyDist
) = randstring(rng, dist.length)
```

```{julia}
model_str = HMM(p, A, MyDist.(1:3))
rand(rng, model_str, 5)
```

::::

:::: {.column width="50%"}

::: {.callout-tip}
## Benefits

Arbitrary observations (strings, point processes, images).
:::

::: {.callout-important}
## Challenges

What is the correct interface for these distributions?
:::

::::

:::::

## Automatic differentiation

- ~~There are mutations, Zygote.jl will never survive~~
- My middle initial is C for ChainRules.jl

::::: {.columns}

:::: {.column width="50%"}

```{julia}
import Zygote

f(μ) = logdensityof(
  HMM(
    [0.9, 0.1], [0.7 0.3; 0.2 0.8],
    Normal.(μ, ones(2))
  ),
  [-1.0, -1.3, -0.6, 1.4, 1.2, 0.9]
)

Zygote.gradient(f, [-2.0, 2.0])
```

::::

:::: {.column width="50%"}

::: {.callout-tip}
## Benefits

Efficient gradient computations.
:::

::: {.callout-important}
## Challenges

Writing down the math.
:::

::::

:::::

## Controls

- ~~My HMM never changes, it's my rock~~
- Bring in the exogenous variables!


```{julia}
import HiddenMarkovModels as HMMs

struct Dirac; val; end
struct DiracHMM <: AbstractHMM; N::Int; end

HMMs.initialization(model::DiracHMM) = ones(model.N) / model.N
HMMs.transition_matrix(model::DiracHMM, control) = ones(model.N, model.N) / model.N
HMMs.obs_distributions(::DiracHMM, control) = Dirac(control)
```

::::: {.columns}

:::: {.column width="50%"}

::: {.callout-tip}
## Benefits

Temporal heterogeneity can be modeled.
:::

::::

:::: {.column width="50%"}

::: {.callout-important}
## Challenges

More complicated estimation methods.
:::

::::

:::::

# More speed

## Type stability

- Essential for Just-In-Time compilation to work well
- Tested with [JET.jl](https://github.com/aviatesk/JET.jl) for all major subroutines

## No allocations

- Crucial for performance in hot loops
- Tested with `@allocated` for all major subroutines

## Multithreading

- Inference and estimation on multiple sequences are embarrassingly parallel
- Implemented with `Threads.@threads`, potential for multithreaded reductions too

# More reliability

## Good practices in package development

- Unit tests
- Documentation with [Documenter.jl](https://github.com/JuliaDocs/Documenter.jl)
- Tutorials with [Literate.jl](https://github.com/JuliaDocs/Documenter.jl)

## Some encouraging benchmarks

![Comparison against Python and Julia competitors](images/benchmark.svg)

# Conclusion

## The role of interfaces

- `AbstractArray` for transition matrices
- [DensityInterface.jl](https://github.com/JuliaMath/DensityInterface.jl) for observation distributions
- [`AbstractHMM`](https://gdalle.github.io/HiddenMarkovModels.jl/stable/api/#HiddenMarkovModels.AbstractHMM) with a handful of functions to handle simulation, inference and learning

::: {.callout-warning}
## Formal specification

For interfaces with precise requirements, "the doc is the API" cannot suffice.
:::


## Publishing software

- Paper in the Journal of Open Source Software [@dalleHiddenMarkovModelsJlGeneric2024]
- Pleasant and productive open review process
- Packages are valuable research contributions
- They need to be cited and recognized

## References

::: {#refs}
:::