---
title: "Fast and generic Hidden Markov Models"
author:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    email: guillaume.dalle@epfl.ch
    affiliation: 
      - name: École Polytechnique Fédérale de Lausanne
      - department: IdePHICS, INDY & SPOC laboratories
date: "2024-07-12"
bibliography: HMM.bib
engine: julia
format:
  revealjs:
    width: 1280
    height: 720
    slide-number: true
    overview: true
    code-line-numbers: false
execute:
    echo: true
---

# Introduction

## About me

- Aka `gdalle` on GitHub & Discourse
- Postdoctoral researcher at EPFL (Switzerland)
- Interested in machine learning and optimization
- Member of [JuliaDiff](https://juliadiff.org/) and [JuliaGraphs](https://juliagraphs.org/)
- Check out my website: <https://gdalle.github.io/>

## What is a Hidden Markov Model?

:::: {.columns}

::: {.column width=65%}
- Statistical model for a temporal process
- Hidden state $X_t$ evolves through time
- Observations $Y_t$ depend on the state
- Basically a noisy Markov chain
:::

::: {.column width="30%"}
![The first Markov chain [@vonhilgersFiveGreatestApplications2006]](./images/eugene_oneguin.png){height=400}
:::

::::

## Applications

- Speech processing
- Bioinformatics
- Predictive maintenance

# The math

## Some notation

:::: {.columns}

::: {.column width="45%"}
- $N$ states, $M$ observations
- Initial probabilities $p$
- Transition matrix $A$
- Emission matrix $B$
:::

::: {.column width="5%"}
:::

::: {.column width="50%"}
$$\mathbb{P}(X_0 = i) = p_i$$
$$\mathbb{P}(X_{t} = j | X_{t-1} = i) = A_{i,j}$$
$$\mathbb{P}(Y_{t} = k | X_t = i) = B_{i,k}$$
:::

::::

## Example: my JuliaCon attendance

:::: {.columns}

::: {.column width="45%"}
```{mermaid}
%%| echo: false
stateDiagram-v2
  direction LR
  state "JuliaCon\nonline" as online
  state "JuliaCon\nin Europe" as europe
  state "JuliaCon\noverseas" as overseas
  online --> overseas: 1/10
  overseas --> online: 1/10
  overseas --> europe: 2/10
  europe --> overseas: 1/10
  overseas --> overseas: 3/10
  online --> online: 2/10
```

$$\pi = \begin{pmatrix} 0 & 0 & 1 \end{pmatrix}$$

$$A = \begin{pmatrix} 2/3 & 1/3 & 0 \\ 1/6 & 3/6 & 2/6 \\ 0 & 1 & 0 \end{pmatrix}$$
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{mermaid}
%%| echo: false
stateDiagram-v2
  state "JuliaCon\nonline" as online
  state "JuliaCon\nin Europe" as europe
  state "JuliaCon\noverseas" as overseas
  state "I'm attending" as attending
  state "I'm missing" as missing
  online --> attending: 3/3
  overseas --> missing: 6/6
  europe --> attending: 1/2
  europe --> missing: 1/2
```

$$B = \begin{pmatrix} 1 & 0 \\ 1/2 & 1/2 \\ 0 & 1 \end{pmatrix}$$
:::

::::

::: {.notes}
2024: Eindhoven
2023: Cambridge MA
2022: Online
2021: Online
2020: Online
2019: Baltimore MD
2018: London
2017: Berkeley CA
2016: Cambridge MA
2015: Cambridge MA
2014: Cambridge MA
:::

## Main problems

Given an HMM, one may want to:

- Computing observation likelihood $\mathbb{P}_\theta(Y_{1:T})$
- Inferring hidden state sequence $\mathbb{P}_\theta(X_{t} | Y_{1:T})$
- Learning parameters $\max_\theta \mathbb{P}_\theta(Y_{1:T})$

See the tutorial by @rabinerTutorialHiddenMarkov1989

# The code

## Basics

Model creation:
```{julia}
using HiddenMarkovModels, Distributions, Random
rng = Random.default_rng()

p = [0.0, 0.0, 1.0]
A = [2/3 1/3 0; 1/6 3/6 2/6; 0 1 0]
B = [1 0; 1/2 1/2; 0 1]
dists = Categorical.(Vector.(eachrow(B)))
model = HMM(p, A, dists);
```
Simulation:
```{julia}
rand(rng, model, 10)
```

## Inference

JuliaCon attendance data:
```{julia}
#| echo: false
state_meaning(i) = if i == 1
  return :online
elseif i == 2
  return :overseas
elseif i == 3
  return :europe
end

obs_meaning(k) = if k == 1
  return :attending
elseif k == 2
  return :missing
end;
```
```{julia}
obs_seq = [2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1]
obs_meaning.(obs_seq)
```

# Why a new package?

## Previous state of the art

- Python:
  - [`hmmlearn`](https://github.com/hmmlearn/hmmlearn) in NumPy
  - [`pomegranate`](https://pomegranate.readthedocs.io/) in PyTorch
  - [`dynamax`](https://github.com/probml/dynamax) in JAX
- Julia: [HMMBase.jl](https://github.com/maxmouchet/HMMBase.jl) 

Need for generality and efficiency

## Number types

::: {.callout-tip}
## Parametric number types

What if I want single precision? Or arbitrary precision? Or logarithmic storage?
:::

Replace `Matrix{Float64}` with `Matrix{<:Real}`.

## Matrix types

::: {.callout-tip}
## Custom 

What if I want single precision? Or arbitrary precision? Or uncertainties?
:::

## Observation types

## Controls

## Automatic differentiationre

# Appendix

## References

::: {#refs}
:::